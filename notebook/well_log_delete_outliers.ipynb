{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51736b7",
   "metadata": {},
   "source": [
    "## 尝试剔除测井曲线的一些异常值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b6f23",
   "metadata": {},
   "source": [
    "除了使用统计方法（如 3σ 法则、箱线图）识别离群点外，我们还可以根据测井原理和地质知识，预先定义一些不合理的数值范围。\n",
    "\n",
    "以下是一些可以先验地认为是异常值的建议和理由：\n",
    "\n",
    "1.  **GR (自然伽马, API)**\n",
    "\n",
    "    - **异常值:** `< 0`\n",
    "    - **理由:** GR 测量的是地层的自然放射性强度，其物理意义决定了它不可能是负值。出现负值通常是由于仪器刻度或数据处理过程中的基线漂移导致的错误。\n",
    "\n",
    "2.  **DEN (或 RHOB, 密度, g/cm³)**\n",
    "\n",
    "    - **异常值:** `< 1.0`\n",
    "    - **理由:**\n",
    "      - 密度值小于 1.0 g/cm³ (水的密度) 是不符合物理规律的，除非在极特殊情况下（如充气井眼）。这通常表示井眼严重扩径（washout），导致密度工具测量到的是密度较低的钻井液而不是地层。\n",
    "\n",
    "3.  **DT (或 AC, 声波时差, us/ft)**\n",
    "\n",
    "    - **异常值:** `< 40` 或 `> 200`\n",
    "    - **理由:**\n",
    "      - 声波时差反映了声波穿过单位距离地层所需的时间。即使是完全致密、无孔隙的岩石（如石英，时差约 55.5 us/ft；白云石，约 43.5 us/ft），其时差值也很难低于 40 us/ft。低于这个值通常是所谓的“跳周”（cycle skipping）现象，是声波仪器在复杂井眼条件下的一种典型错误。\n",
    "      - 大于 200 us/ft 的值表示地层非常疏松或声速极慢。虽然钻井液的时差大约在 189 us/ft，井眼垮塌区域可能出现高值，但持续高于 200 us/ft 的数据点很可能是异常的，特别是在非欠压实地层中。\n",
    "\n",
    "4.  **CAL (井径, in)**\n",
    "\n",
    "    - **异常值:** 小于钻头尺寸 (e.g., `< 8.5`) 或 远大于钻头尺寸 (e.g., `> 16`)\n",
    "    - **理由:**\n",
    "      - 井径曲线反映了井眼的尺寸。它的值理论上不应小于钻井时使用的钻头尺寸（例如，在 8.5 英寸的井眼中，CAL 不应小于 8.5）。小于钻头尺寸的值可能表示仪器问题或井壁上形成了厚厚的泥饼。\n",
    "      - 远大于钻头尺寸的值表示井眼严重扩径或存在大的溶洞。虽然扩径是正常现象，但一个过大的阈值（如 16 英寸）可以帮助我们识别那些可能对其他测井响应产生严重影响的层段。\n",
    "\n",
    "5.  **LLD (深侧向电阻率, ohm.m)**\n",
    "    - **异常值:** `< 0.1` 或 `> 2000`\n",
    "    - **理由:**\n",
    "      - 电阻率不应为负值，任何负值都是错误。极低的值（如小于 0.1 ohm.m）虽然在某些高矿化度盐水层中可能出现，但在常规油气藏勘探中非常罕见，通常表示仪器短路或数据错误。\n",
    "      - 电阻率可以非常高（如在致密碳酸盐岩、煤层、油气层中）。设置一个上限（如 2000 ohm.m）主要是为了处理仪器超出量程或“无穷大”的情况。在对数坐标下，这些极高值会严重影响可视化和统计分析，通常会将它们限制（clip）在一个合理的最高值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e400ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import lasio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 添加src目录到路径\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from utils.well_log_outlier_detector import (\n",
    "    ANOMALY_RULES,\n",
    "    apply_statistical_filter_3sigma,\n",
    "    apply_statistical_filter_iqr,\n",
    "    detect_anomalies,\n",
    ")\n",
    "\n",
    "# 设置中文字体支持\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\", \"DejaVu Sans\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ce040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义路径\n",
    "las_folder = \"../data/vertical_well_truncated_las\"\n",
    "output_folder = \"../data/vertical_well_las_delete_outliers\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "output_path = Path(output_folder)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 配置异常值检测参数\n",
    "DETECTION_CONFIG = {\n",
    "    \"use_3sigma\": True,  # 是否使用3σ法则\n",
    "    \"use_iqr\": True,  # 是否使用IQR方法\n",
    "    \"sigma\": 3.0,  # 3σ法则的标准差倍数\n",
    "    \"iqr_multiplier\": 3.0,  # IQR方法的倍数(1.5为温和离群点,3.0为极端离群点)\n",
    "    \"combine_method\": \"intersection\",  # 统计方法组合方式: \"union\"(并集)或\"intersection\"(交集)\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"测井曲线异常值处理\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n异常值规则:\")\n",
    "for curve, rule in ANOMALY_RULES.items():\n",
    "    min_str = f\"{rule['min']}\" if rule[\"min\"] is not None else \"无限制\"\n",
    "    max_str = f\"{rule['max']}\" if rule[\"max\"] is not None else \"无限制\"\n",
    "    print(f\"  {curve:8s}: [{min_str:8s}, {max_str:8s}] - {rule['description']}\")\n",
    "\n",
    "print(\"\\n统计方法配置:\")\n",
    "print(f\"  3σ法则: {'启用' if DETECTION_CONFIG['use_3sigma'] else '禁用'} (σ={DETECTION_CONFIG['sigma']})\")\n",
    "print(f\"  IQR方法: {'启用' if DETECTION_CONFIG['use_iqr'] else '禁用'} (倍数={DETECTION_CONFIG['iqr_multiplier']})\")\n",
    "print(f\"  组合方式: {DETECTION_CONFIG['combine_method']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_combined(\n",
    "    data: np.ndarray,\n",
    "    rule: Optional[Dict] = None,\n",
    "    use_3sigma: bool = True,\n",
    "    use_iqr: bool = True,\n",
    "    sigma: float = 3.0,\n",
    "    iqr_multiplier: float = 1.5,\n",
    "    combine_method: str = \"union\",\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[np.ndarray, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    综合多种方法检测异常值\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray\n",
    "        测井数据\n",
    "    rule : dict, optional\n",
    "        先验规则,格式: {\"min\": float, \"max\": float}\n",
    "    use_3sigma : bool, default=True\n",
    "        是否使用3σ法则\n",
    "    use_iqr : bool, default=True\n",
    "        是否使用IQR方法\n",
    "    sigma : float, default=3.0\n",
    "        3σ法则的标准差倍数\n",
    "    iqr_multiplier : float, default=1.5\n",
    "        IQR方法的倍数\n",
    "    combine_method : str, default=\"union\"\n",
    "        统计方法组合方式: \"union\"(并集)或\"intersection\"(交集)\n",
    "    verbose : bool, default=True\n",
    "        是否打印详细信息\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    total_mask : np.ndarray\n",
    "        总异常值掩码\n",
    "    stats : dict\n",
    "        各类异常值统计信息\n",
    "    \"\"\"\n",
    "    original_count = len(data)\n",
    "    stats = {\"original_count\": original_count}\n",
    "\n",
    "    # 1. 先验规则检测\n",
    "    if rule is not None:\n",
    "        prior_mask = detect_anomalies(data, min_val=rule.get(\"min\"), max_val=rule.get(\"max\"))\n",
    "        prior_count = np.sum(prior_mask)\n",
    "        stats[\"prior_count\"] = prior_count\n",
    "        stats[\"prior_pct\"] = (prior_count / original_count) * 100\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  先验异常值: {prior_count} ({stats['prior_pct']:.2f}%)\")\n",
    "    else:\n",
    "        prior_mask = np.zeros(len(data), dtype=bool)\n",
    "        stats[\"prior_count\"] = 0\n",
    "        stats[\"prior_pct\"] = 0.0  # type: ignore\n",
    "\n",
    "    # 2. 统计方法检测(仅对非先验异常值应用)\n",
    "    valid_data_mask = ~prior_mask\n",
    "    statistical_mask = np.zeros(len(data), dtype=bool)\n",
    "\n",
    "    if np.sum(valid_data_mask) > 0:\n",
    "        if use_3sigma and use_iqr:\n",
    "            # 同时使用两种方法\n",
    "            sigma_mask = apply_statistical_filter_3sigma(data, sigma=sigma, verbose=verbose)\n",
    "            iqr_mask = apply_statistical_filter_iqr(data, multiplier=iqr_multiplier, verbose=verbose)\n",
    "\n",
    "            if combine_method == \"union\":\n",
    "                # 并集:任一方法认为是离群点即标记\n",
    "                statistical_mask = sigma_mask | iqr_mask\n",
    "            else:  # intersection\n",
    "                # 交集:两种方法都认为是离群点才标记\n",
    "                statistical_mask = sigma_mask & iqr_mask\n",
    "\n",
    "            stats[\"3sigma_count\"] = np.sum(sigma_mask & valid_data_mask)  # type: ignore\n",
    "            stats[\"iqr_count\"] = np.sum(iqr_mask & valid_data_mask)  # type: ignore\n",
    "\n",
    "        elif use_3sigma:\n",
    "            statistical_mask = apply_statistical_filter_3sigma(data, sigma=sigma, verbose=verbose)\n",
    "            stats[\"3sigma_count\"] = np.sum(statistical_mask & valid_data_mask)  # type: ignore\n",
    "\n",
    "        elif use_iqr:\n",
    "            statistical_mask = apply_statistical_filter_iqr(data, multiplier=iqr_multiplier, verbose=verbose)\n",
    "            stats[\"iqr_count\"] = np.sum(statistical_mask & valid_data_mask)  # type: ignore\n",
    "\n",
    "        # 仅统计非先验异常的统计离群值\n",
    "        statistical_only_mask = statistical_mask & valid_data_mask\n",
    "        stats[\"statistical_count\"] = np.sum(statistical_only_mask)  # type: ignore\n",
    "        stats[\"statistical_pct\"] = (stats[\"statistical_count\"] / original_count) * 100  # type: ignore\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  统计离群值: {stats['statistical_count']} ({stats['statistical_pct']:.2f}%)\")\n",
    "    else:\n",
    "        stats[\"statistical_count\"] = 0\n",
    "        stats[\"statistical_pct\"] = 0.0  # type: ignore\n",
    "\n",
    "    # 3. 合并所有异常值\n",
    "    total_mask = prior_mask | statistical_mask\n",
    "    stats[\"total_count\"] = np.sum(total_mask)  # type: ignore\n",
    "    stats[\"total_pct\"] = (stats[\"total_count\"] / original_count) * 100  # type: ignore\n",
    "    stats[\"valid_count\"] = original_count - stats[\"total_count\"]  # type: ignore\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  总异常值: {stats['total_count']} ({stats['total_pct']:.2f}%)\")\n",
    "        print(f\"  有效数据点数: {stats['valid_count']}\")\n",
    "\n",
    "    return total_mask, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eebb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有LAS文件\n",
    "input_path = Path(las_folder)\n",
    "las_files = list(input_path.glob(\"*.las\"))\n",
    "las_files = list(set(las_files))\n",
    "las_files.sort()\n",
    "\n",
    "if not las_files:\n",
    "    print(f\"\\n在 {las_folder} 中未找到LAS文件\")\n",
    "else:\n",
    "    print(f\"\\n找到 {len(las_files)} 个LAS文件\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 统计信息\n",
    "    processing_stats = []\n",
    "    success_count = 0\n",
    "    failed_files = []\n",
    "\n",
    "    for las_file in las_files:\n",
    "        well_name = las_file.stem\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"处理井: {well_name}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "\n",
    "        try:\n",
    "            # 读取LAS文件\n",
    "            las = lasio.read(las_file, mnemonic_case=\"upper\")\n",
    "\n",
    "            # 统计该井的异常值信息\n",
    "            well_stats = {\"Well\": well_name}\n",
    "\n",
    "            # 创建新的LAS对象\n",
    "            new_las = lasio.LASFile()\n",
    "            new_las.version = las.version\n",
    "            new_las.well = las.well\n",
    "            new_las.params = las.params\n",
    "            new_las.other = las.other\n",
    "\n",
    "            # 处理每条曲线\n",
    "            for curve in las.curves:\n",
    "                mnemonic = curve.mnemonic\n",
    "                data = curve.data.copy()\n",
    "                original_count = len(data)\n",
    "\n",
    "                # 检查是否有对应的异常值规则\n",
    "                rule = ANOMALY_RULES.get(mnemonic)\n",
    "\n",
    "                if rule is not None:\n",
    "                    print(f\"\\n曲线: {mnemonic}\")\n",
    "                    print(f\"  原始数据点数: {original_count}\")\n",
    "\n",
    "                    # 使用综合检测方法\n",
    "                    total_mask, stats = detect_outliers_combined(data=data, rule=rule, **DETECTION_CONFIG, verbose=True)\n",
    "\n",
    "                    # 将异常值设为NaN\n",
    "                    data[total_mask] = np.nan\n",
    "\n",
    "                    # 记录统计信息\n",
    "                    well_stats[f\"{mnemonic}_Points\"] = stats[\"original_count\"]  # type: ignore\n",
    "                    well_stats[f\"{mnemonic}_Prior_Count\"] = stats[\"prior_count\"]  # type: ignore\n",
    "                    well_stats[f\"{mnemonic}_Prior_Pct\"] = stats[\"prior_pct\"]  # type: ignore\n",
    "                    well_stats[f\"{mnemonic}_Statistical_Count\"] = stats[\"statistical_count\"]  # type: ignore\n",
    "                    well_stats[f\"{mnemonic}_Statistical_Pct\"] = stats[\"statistical_pct\"]  # type: ignore\n",
    "                    well_stats[f\"{mnemonic}_Total_Count\"] = stats[\"total_count\"]  # type: ignore\n",
    "                    well_stats[f\"{mnemonic}_Total_Pct\"] = stats[\"total_pct\"]  # type: ignore\n",
    "                    well_stats[f\"{mnemonic}_Valid_Count\"] = stats[\"valid_count\"]  # type: ignore\n",
    "\n",
    "                    # 如果同时使用了3σ和IQR,记录各自的统计\n",
    "                    if \"3sigma_count\" in stats:\n",
    "                        well_stats[f\"{mnemonic}_3Sigma_Count\"] = stats[\"3sigma_count\"]  # type: ignore\n",
    "                    if \"iqr_count\" in stats:\n",
    "                        well_stats[f\"{mnemonic}_IQR_Count\"] = stats[\"iqr_count\"]  # type: ignore\n",
    "\n",
    "                else:\n",
    "                    # 对于没有先验规则的曲线,只进行基本的NaN和Inf检测\n",
    "                    basic_mask = np.isnan(data) | np.isinf(data)\n",
    "                    basic_anomaly_count = np.sum(basic_mask)\n",
    "\n",
    "                    if basic_anomaly_count > 0:\n",
    "                        print(f\"\\n曲线: {mnemonic}\")\n",
    "                        print(f\"  原始数据点数: {original_count}\")\n",
    "                        print(f\"  NaN/Inf异常: {basic_anomaly_count}\")\n",
    "                        data[basic_mask] = np.nan\n",
    "\n",
    "                # 添加处理后的曲线\n",
    "                new_las.append_curve(\n",
    "                    mnemonic=curve.mnemonic,\n",
    "                    data=data,\n",
    "                    unit=curve.unit,\n",
    "                    descr=curve.descr,\n",
    "                    value=curve.value,\n",
    "                )\n",
    "\n",
    "            # 保存处理后的文件\n",
    "            output_file = output_path / las_file.name\n",
    "            new_las.write(str(output_file), version=2.0)\n",
    "\n",
    "            print(f\"\\n✓ 成功处理并保存到 {output_file.name}\")\n",
    "            success_count += 1\n",
    "\n",
    "            # 保存统计信息\n",
    "            processing_stats.append(well_stats)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ 处理失败: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "            failed_files.append((well_name, str(e)))\n",
    "\n",
    "    # 保存处理统计信息\n",
    "    if processing_stats:\n",
    "        stats_df = pd.DataFrame(processing_stats)\n",
    "        stats_file = \"output/processing_statistics.xlsx\"\n",
    "        stats_df.to_excel(stats_file, index=False)\n",
    "        print(f\"\\n统计信息已保存到: {stats_file}\")\n",
    "\n",
    "    # 输出总体统计\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"处理完成!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"成功: {success_count}/{len(las_files)}\")\n",
    "\n",
    "    if failed_files:\n",
    "        print(f\"\\n失败的文件:\")\n",
    "        for filename, error in failed_files:\n",
    "            print(f\"  - {filename}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化统计结果\n",
    "if processing_stats:\n",
    "    stats_df = pd.DataFrame(processing_stats)\n",
    "\n",
    "    # 统计各曲线的异常值情况\n",
    "    curves_to_analyze = [\"GR\", \"DEN\", \"RHOB\", \"DT\", \"AC\", \"CAL\", \"CALI\", \"LLD\", \"LLD1\"]\n",
    "\n",
    "    # 提取异常值百分比数据用于热力图\n",
    "    anomaly_pct_data = {}\n",
    "    for curve in curves_to_analyze:\n",
    "        pct_col = f\"{curve}_Total_Pct\"\n",
    "        if pct_col in stats_df.columns:\n",
    "            anomaly_pct_data[curve] = stats_df[pct_col].fillna(0).values\n",
    "\n",
    "    # 提取异常值数量数据用于柱状图\n",
    "    anomaly_counts = {}\n",
    "    for curve in curves_to_analyze:\n",
    "        count_col = f\"{curve}_Total_Count\"\n",
    "        if count_col in stats_df.columns:\n",
    "            anomaly_counts[curve] = stats_df[count_col].fillna(0).values\n",
    "\n",
    "    # 创建图表 (1行2列布局)\n",
    "    fig = plt.figure(figsize=(20, 8))\n",
    "    gs = fig.add_gridspec(1, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # 1. 热力图:各井各曲线异常值占比\n",
    "    if anomaly_pct_data:\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        pct_df = pd.DataFrame(anomaly_pct_data, index=stats_df[\"Well\"])\n",
    "        pct_df_filtered = pct_df.loc[:, (pct_df > 0).any()]\n",
    "\n",
    "        if len(pct_df_filtered.columns) > 0:\n",
    "            im = ax1.imshow(pct_df_filtered.values, cmap=\"YlOrRd\", aspect=\"auto\", vmin=0, vmax=100)\n",
    "            ax1.set_xticks(range(len(pct_df_filtered.columns)))\n",
    "            ax1.set_xticklabels(pct_df_filtered.columns, fontsize=11, fontweight=\"bold\")\n",
    "            ax1.set_yticks(range(len(pct_df_filtered.index)))\n",
    "            ax1.set_yticklabels([name[:15] for name in pct_df_filtered.index], fontsize=9)\n",
    "            ax1.set_title(\"各井各曲线异常值占比热力图 (%)\", fontweight=\"bold\", fontsize=14, pad=15)\n",
    "            ax1.set_xlabel(\"曲线类型\", fontsize=12, fontweight=\"bold\")\n",
    "            ax1.set_ylabel(\"井名\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "            # 添加颜色条\n",
    "            cbar = plt.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
    "            cbar.set_label(\"异常值占比 (%)\", rotation=270, labelpad=20, fontsize=11)\n",
    "\n",
    "            # 在热力图上标注数值\n",
    "            for i in range(len(pct_df_filtered.index)):\n",
    "                for j in range(len(pct_df_filtered.columns)):\n",
    "                    value = pct_df_filtered.values[i, j]\n",
    "                    text_color = \"white\" if value > 50 else \"black\"\n",
    "                    ax1.text(\n",
    "                        j,\n",
    "                        i,\n",
    "                        f\"{value:.1f}\",\n",
    "                        ha=\"center\",\n",
    "                        va=\"center\",\n",
    "                        color=text_color,\n",
    "                        fontsize=8,\n",
    "                        fontweight=\"bold\",\n",
    "                    )\n",
    "\n",
    "    # 2. 堆叠柱状图:各井异常值数量分解\n",
    "    if anomaly_counts:\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        counts_df = pd.DataFrame(anomaly_counts, index=stats_df[\"Well\"])\n",
    "        counts_df_filtered = counts_df.loc[:, (counts_df > 0).any()]\n",
    "\n",
    "        if len(counts_df_filtered.columns) > 0:\n",
    "            counts_df_filtered.plot(kind=\"bar\", stacked=True, ax=ax2, colormap=\"tab10\", width=0.8)\n",
    "            ax2.set_title(\"各井异常值数量分解\", fontweight=\"bold\", fontsize=13)\n",
    "            ax2.set_xlabel(\"井名\", fontsize=11)\n",
    "            ax2.set_ylabel(\"异常值数量\", fontsize=11)\n",
    "            ax2.legend(title=\"曲线\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=9)\n",
    "            ax2.set_xticklabels([name[:12] for name in counts_df_filtered.index], rotation=45, ha=\"right\", fontsize=9)\n",
    "            ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    fig.suptitle(\"测井曲线异常值统计分析\", fontsize=16, fontweight=\"bold\", y=0.98)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存图表\n",
    "    fig_file = \"output/anomaly_statistics.png\"\n",
    "    plt.savefig(fig_file, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"\\n统计图表已保存到: {fig_file}\")\n",
    "\n",
    "    # 输出统计摘要\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"异常值统计摘要\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if anomaly_pct_data:\n",
    "        pct_df = pd.DataFrame(anomaly_pct_data, index=stats_df[\"Well\"])\n",
    "        print(\"\\n各曲线平均异常值占比:\")\n",
    "        for col in pct_df.columns:\n",
    "            if pct_df[col].sum() > 0:\n",
    "                avg_pct = pct_df[col].mean()\n",
    "                max_pct = pct_df[col].max()\n",
    "                max_well = pct_df[col].idxmax()\n",
    "                print(f\"  {col:8s}: 平均 {avg_pct:5.2f}%  最大 {max_pct:5.2f}% ({max_well})\")\n",
    "\n",
    "        print(\"\\n异常值占比最高的5口井:\")\n",
    "        well_avg = pct_df.mean(axis=1).sort_values(ascending=False).head(5)\n",
    "        for well, avg in well_avg.items():\n",
    "            print(f\"  {well:20s}: {avg:5.2f}%\")\n",
    "\n",
    "    if anomaly_counts:\n",
    "        counts_df = pd.DataFrame(anomaly_counts, index=stats_df[\"Well\"])\n",
    "        counts_df_filtered = counts_df.loc[:, (counts_df > 0).any()]\n",
    "        if len(counts_df_filtered.columns) > 0:\n",
    "            print(\"\\n各曲线总异常值数量:\")\n",
    "            for col in counts_df_filtered.columns:\n",
    "                total_count = counts_df_filtered[col].sum()\n",
    "                max_count = counts_df_filtered[col].max()\n",
    "                max_well = counts_df_filtered[col].idxmax()\n",
    "                print(f\"  {col:8s}: 总计 {total_count:6.0f}  最大 {max_count:6.0f} ({max_well})\")\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transform2021_devito",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
