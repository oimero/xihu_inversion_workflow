{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e86bba",
   "metadata": {},
   "source": [
    "## 尝试分层剔除测井曲线的一些异常值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644bb5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import lasio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 添加src目录到路径\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from utils.well_log_outlier_detector import (\n",
    "    ANOMALY_RULES,\n",
    "    apply_statistical_filter_3sigma,\n",
    "    apply_statistical_filter_iqr,\n",
    "    detect_anomalies,\n",
    ")\n",
    "\n",
    "# 设置中文字体支持\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\", \"DejaVu Sans\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 配置参数 ============\n",
    "\n",
    "# 定义路径\n",
    "LAS_FOLDER = \"../data/vertical_well_truncated_las\"\n",
    "HORIZON_FILE = \"../data/well_horizon_processed.xlsx\"\n",
    "OUTPUT_FOLDER = \"../data/vertical_well_las_delete_outliers_by_layers\"\n",
    "\n",
    "# 创建输出文件夹\n",
    "output_path = Path(OUTPUT_FOLDER)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 定义砂组层序（从上到下）\n",
    "SAND_GROUPS = [\n",
    "    \"H1-1\",\n",
    "    \"H2-1\",\n",
    "    \"H3-1\",\n",
    "    \"H4-1\",\n",
    "    \"H5-1\",\n",
    "    \"H6-1\",\n",
    "    \"H7-1\",\n",
    "    \"H8-1\",\n",
    "    \"P0\",\n",
    "]\n",
    "\n",
    "# 异常值检测配置\n",
    "DETECTION_CONFIG = {\n",
    "    \"use_3sigma\": True,  # 是否使用3σ法则\n",
    "    \"use_iqr\": True,  # 是否使用IQR方法\n",
    "    \"sigma\": 3.0,  # 3σ法则的标准差倍数\n",
    "    \"iqr_multiplier\": 1.5,  # IQR方法的倍数(1.5为温和离群点,3.0为极端离群点)\n",
    "    \"combine_method\": \"intersection\",  # 统计方法组合方式: \"union\"(并集)或\"intersection\"(交集)\n",
    "    \"min_samples\": 100,  # 层段最小样本量阈值\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"分层处理测井曲线异常值\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n异常值规则:\")\n",
    "for curve, rule in ANOMALY_RULES.items():\n",
    "    min_str = f\"{rule['min']}\" if rule[\"min\"] is not None else \"无限制\"\n",
    "    max_str = f\"{rule['max']}\" if rule[\"max\"] is not None else \"无限制\"\n",
    "    print(f\"  {curve:8s}: [{min_str:8s}, {max_str:8s}] - {rule['description']}\")\n",
    "\n",
    "print(\"\\n统计方法配置:\")\n",
    "print(f\"  3σ法则: {'启用' if DETECTION_CONFIG['use_3sigma'] else '禁用'} (σ={DETECTION_CONFIG['sigma']})\")\n",
    "print(f\"  IQR方法: {'启用' if DETECTION_CONFIG['use_iqr'] else '禁用'} (倍数={DETECTION_CONFIG['iqr_multiplier']})\")\n",
    "print(f\"  组合方式: {DETECTION_CONFIG['combine_method']}\")\n",
    "print(f\"  最小样本量: {DETECTION_CONFIG['min_samples']}\")\n",
    "print(f\"\\n层位顺序: {' → '.join(SAND_GROUPS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d83ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 分层异常值检测函数 ============\n",
    "\n",
    "\n",
    "def detect_outliers_by_geological_layers(\n",
    "    data: np.ndarray,\n",
    "    depth: np.ndarray,\n",
    "    horizon_df: pd.DataFrame,\n",
    "    well_name: str,\n",
    "    sand_groups: list,\n",
    "    rule: Optional[Dict] = None,\n",
    "    use_3sigma: bool = True,\n",
    "    use_iqr: bool = True,\n",
    "    sigma: float = 3.0,\n",
    "    iqr_multiplier: float = 1.5,\n",
    "    combine_method: str = \"union\",\n",
    "    min_samples: int = 100,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    按地质层段分层检测异常值\n",
    "\n",
    "    工作流程:\n",
    "    1. 先用先验规则进行全局检测\n",
    "    2. 根据层位数据将井段划分为多个层段\n",
    "    3. 在每个层段内独立进行统计学异常值检测\n",
    "    4. 合并所有异常值\n",
    "\n",
    "    层段划分规则:\n",
    "    - 每个层位的深度作为该层段的顶界,延伸到下一层位\n",
    "    - 最顶层位以上和最底层位以下的数据不参与处理\n",
    "    - 每个层位上的数据点归属于其下方的层段\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray\n",
    "        测井数据\n",
    "    depth : np.ndarray\n",
    "        深度数据\n",
    "    horizon_df : pd.DataFrame\n",
    "        层位数据表,必须包含列: ['Well', 'Surface', 'MD']\n",
    "    well_name : str\n",
    "        井名\n",
    "    sand_groups : list\n",
    "        砂组层序列表(从上到下)\n",
    "    rule : dict, optional\n",
    "        先验规则,格式: {\"min\": float, \"max\": float, \"description\": str}\n",
    "    use_3sigma : bool\n",
    "        是否使用3σ法则\n",
    "    use_iqr : bool\n",
    "        是否使用IQR方法\n",
    "    sigma : float\n",
    "        3σ法则的标准差倍数\n",
    "    iqr_multiplier : float\n",
    "        IQR方法的倍数\n",
    "    combine_method : str\n",
    "        统计方法组合方式: \"union\"或\"intersection\"\n",
    "    min_samples : int\n",
    "        层段最小样本量阈值\n",
    "    verbose : bool\n",
    "        是否打印详细信息\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    total_mask : np.ndarray\n",
    "        总异常值掩码(True表示异常)\n",
    "    stats : dict\n",
    "        详细统计信息\n",
    "    \"\"\"\n",
    "    # 初始化\n",
    "    original_count = len(data)\n",
    "    total_mask = np.zeros(len(data), dtype=bool)\n",
    "\n",
    "    stats = {\n",
    "        \"original_count\": original_count,\n",
    "        \"prior_count\": 0,\n",
    "        \"prior_pct\": 0.0,\n",
    "        \"statistical_count\": 0,\n",
    "        \"statistical_pct\": 0.0,\n",
    "        \"total_count\": 0,\n",
    "        \"total_pct\": 0.0,\n",
    "        \"valid_count\": 0,\n",
    "        \"layers_processed\": 0,\n",
    "        \"layers_skipped\": 0,\n",
    "        \"layer_details\": [],\n",
    "    }\n",
    "\n",
    "    # ========== 1. 先验规则检测(全局) ==========\n",
    "    if rule is not None:\n",
    "        prior_mask = detect_anomalies(data, min_val=rule.get(\"min\"), max_val=rule.get(\"max\"))\n",
    "        prior_count = np.sum(prior_mask)\n",
    "        stats[\"prior_count\"] = prior_count\n",
    "        stats[\"prior_pct\"] = (prior_count / original_count) * 100 if original_count > 0 else 0\n",
    "        total_mask |= prior_mask\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  先验异常值(全局): {prior_count} ({stats['prior_pct']:.2f}%)\")\n",
    "\n",
    "    # ========== 2. 获取该井的层位数据 ==========\n",
    "    well_horizons = horizon_df[horizon_df[\"Well\"] == well_name].copy()\n",
    "\n",
    "    if len(well_horizons) == 0:\n",
    "        if verbose:\n",
    "            print(f\"  ⚠️ 未找到井 {well_name} 的层位数据,跳过分层处理\")\n",
    "        stats[\"statistical_count\"] = 0\n",
    "        stats[\"statistical_pct\"] = 0.0\n",
    "        stats[\"total_count\"] = stats[\"prior_count\"]\n",
    "        stats[\"total_pct\"] = stats[\"prior_pct\"]\n",
    "        stats[\"valid_count\"] = original_count - stats[\"total_count\"]\n",
    "        return total_mask, stats\n",
    "\n",
    "    # 筛选出存在的砂组层位并按深度排序\n",
    "    well_sand_groups = well_horizons[well_horizons[\"Surface\"].isin(sand_groups)].copy()\n",
    "    well_sand_groups = well_sand_groups.sort_values(\"MD\").reset_index(drop=True)\n",
    "\n",
    "    if len(well_sand_groups) < 2:\n",
    "        if verbose:\n",
    "            print(f\"  ⚠️ 井 {well_name} 的层位数少于2个,无法分层处理\")\n",
    "        stats[\"statistical_count\"] = 0\n",
    "        stats[\"statistical_pct\"] = 0.0\n",
    "        stats[\"total_count\"] = stats[\"prior_count\"]\n",
    "        stats[\"total_pct\"] = stats[\"prior_pct\"]\n",
    "        stats[\"valid_count\"] = original_count - stats[\"total_count\"]\n",
    "        return total_mask, stats\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  找到 {len(well_sand_groups)} 个层位\")\n",
    "        print(\n",
    "            f\"  深度范围: {well_sand_groups['Surface'].iloc[0]} ({well_sand_groups['MD'].iloc[0]:.2f}m) \"\n",
    "            f\"→ {well_sand_groups['Surface'].iloc[-1]} ({well_sand_groups['MD'].iloc[-1]:.2f}m)\"\n",
    "        )\n",
    "\n",
    "    # ========== 3. 按层段处理 ==========\n",
    "    statistical_mask = np.zeros(len(data), dtype=bool)\n",
    "\n",
    "    # 遍历每个层段(从第一个层位到倒数第二个层位)\n",
    "    for i in range(len(well_sand_groups) - 1):\n",
    "        top_surface = well_sand_groups.iloc[i][\"Surface\"]\n",
    "        bottom_surface = well_sand_groups.iloc[i + 1][\"Surface\"]\n",
    "        top_md = well_sand_groups.iloc[i][\"MD\"]\n",
    "        bottom_md = well_sand_groups.iloc[i + 1][\"MD\"]\n",
    "\n",
    "        # 获取该层段的数据掩码\n",
    "        # 注意: 包含顶界点,不包含底界点(底界点属于下一层段)\n",
    "        layer_mask = (depth >= top_md) & (depth < bottom_md)\n",
    "        layer_count = np.sum(layer_mask)\n",
    "\n",
    "        layer_info = {\n",
    "            \"layer\": f\"{top_surface} → {bottom_surface}\",\n",
    "            \"top_surface\": top_surface,\n",
    "            \"bottom_surface\": bottom_surface,\n",
    "            \"top_md\": top_md,\n",
    "            \"bottom_md\": bottom_md,\n",
    "            \"thickness\": bottom_md - top_md,\n",
    "            \"total_points\": layer_count,\n",
    "            \"processed\": False,\n",
    "            \"skip_reason\": None,\n",
    "            \"prior_outliers\": 0,\n",
    "            \"statistical_outliers\": 0,\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n  --- 层段 {i + 1}/{len(well_sand_groups) - 1}: {top_surface} → {bottom_surface} ---\")\n",
    "            print(f\"      深度: {top_md:.2f}m - {bottom_md:.2f}m (厚度: {bottom_md - top_md:.2f}m)\")\n",
    "            print(f\"      数据点数: {layer_count}\")\n",
    "\n",
    "        # 检查样本量\n",
    "        if layer_count < min_samples:\n",
    "            if verbose:\n",
    "                print(f\"      ⚠️ 样本量不足({layer_count} < {min_samples}),跳过该层段\")\n",
    "            layer_info[\"skip_reason\"] = f\"样本量不足({layer_count} < {min_samples})\"\n",
    "            stats[\"layers_skipped\"] += 1\n",
    "            stats[\"layer_details\"].append(layer_info)\n",
    "            continue\n",
    "\n",
    "        # 提取该层段数据\n",
    "        layer_data = data[layer_mask]\n",
    "\n",
    "        # 统计该层段中的先验异常值\n",
    "        layer_prior_mask = total_mask[layer_mask]\n",
    "        layer_prior_count = np.sum(layer_prior_mask)\n",
    "        layer_info[\"prior_outliers\"] = layer_prior_count\n",
    "\n",
    "        # 计算去除先验异常值后的有效样本量\n",
    "        valid_layer_mask = ~layer_prior_mask\n",
    "        valid_layer_count = np.sum(valid_layer_mask)\n",
    "\n",
    "        if verbose and layer_prior_count > 0:\n",
    "            print(f\"      先验异常值: {layer_prior_count} ({(layer_prior_count / layer_count) * 100:.2f}%)\")\n",
    "            print(f\"      有效样本: {valid_layer_count}\")\n",
    "\n",
    "        if valid_layer_count < min_samples:\n",
    "            if verbose:\n",
    "                print(f\"      ⚠️ 去除先验异常值后样本量不足({valid_layer_count} < {min_samples}),跳过\")\n",
    "            layer_info[\"skip_reason\"] = f\"有效样本不足({valid_layer_count} < {min_samples})\"\n",
    "            stats[\"layers_skipped\"] += 1\n",
    "            stats[\"layer_details\"].append(layer_info)\n",
    "            continue\n",
    "\n",
    "        # ========== 在该层段内进行统计检测 ==========\n",
    "        layer_statistical_mask = np.zeros(layer_count, dtype=bool)\n",
    "\n",
    "        # 根据配置选择统计方法\n",
    "        if use_3sigma and use_iqr:\n",
    "            # 同时使用两种方法\n",
    "            sigma_mask = apply_statistical_filter_3sigma(layer_data, sigma=sigma, verbose=verbose)\n",
    "            iqr_mask = apply_statistical_filter_iqr(layer_data, multiplier=iqr_multiplier, verbose=verbose)\n",
    "\n",
    "            if combine_method == \"union\":\n",
    "                layer_statistical_mask = sigma_mask | iqr_mask\n",
    "            else:  # intersection\n",
    "                layer_statistical_mask = sigma_mask & iqr_mask\n",
    "\n",
    "        elif use_3sigma:\n",
    "            layer_statistical_mask = apply_statistical_filter_3sigma(layer_data, sigma=sigma, verbose=verbose)\n",
    "\n",
    "        elif use_iqr:\n",
    "            layer_statistical_mask = apply_statistical_filter_iqr(\n",
    "                layer_data, multiplier=iqr_multiplier, verbose=verbose\n",
    "            )\n",
    "\n",
    "        # 只标记非先验异常的统计离群值\n",
    "        layer_statistical_only = layer_statistical_mask & valid_layer_mask\n",
    "        statistical_count_in_layer = np.sum(layer_statistical_only)\n",
    "\n",
    "        layer_info[\"statistical_outliers\"] = statistical_count_in_layer\n",
    "        layer_info[\"processed\"] = True\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"      统计离群值: {statistical_count_in_layer} ({(statistical_count_in_layer / layer_count) * 100:.2f}%)\"\n",
    "            )\n",
    "\n",
    "        # 更新全局掩码\n",
    "        global_indices = np.where(layer_mask)[0]\n",
    "        statistical_mask[global_indices[layer_statistical_only]] = True\n",
    "\n",
    "        stats[\"layers_processed\"] += 1\n",
    "        stats[\"layer_details\"].append(layer_info)\n",
    "\n",
    "    # ========== 4. 合并所有异常值 ==========\n",
    "    total_mask |= statistical_mask\n",
    "\n",
    "    stats[\"statistical_count\"] = np.sum(statistical_mask)\n",
    "    stats[\"statistical_pct\"] = (stats[\"statistical_count\"] / original_count) * 100 if original_count > 0 else 0\n",
    "    stats[\"total_count\"] = np.sum(total_mask)\n",
    "    stats[\"total_pct\"] = (stats[\"total_count\"] / original_count) * 100 if original_count > 0 else 0\n",
    "    stats[\"valid_count\"] = original_count - stats[\"total_count\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n  === 汇总 ===\")\n",
    "        print(f\"  处理层段数: {stats['layers_processed']}\")\n",
    "        print(f\"  跳过层段数: {stats['layers_skipped']}\")\n",
    "        print(f\"  统计离群值(分层): {stats['statistical_count']} ({stats['statistical_pct']:.2f}%)\")\n",
    "        print(f\"  总异常值: {stats['total_count']} ({stats['total_pct']:.2f}%)\")\n",
    "        print(f\"  有效数据点数: {stats['valid_count']}\")\n",
    "\n",
    "    return total_mask, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cacdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 读取层位数据 ============\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"读取层位数据...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "horizon_df = pd.read_excel(HORIZON_FILE)\n",
    "# print(f\"层位数据: {len(horizon_df)} 条记录\")\n",
    "# print(f\"包含的井: {sorted(horizon_df['Well'].unique())}\")\n",
    "\n",
    "# # 统计每口井的层位数\n",
    "# well_horizon_counts = horizon_df.groupby(\"Well\").size()\n",
    "# print(f\"\\n各井层位数量:\")\n",
    "# for well, count in well_horizon_counts.items():\n",
    "#     print(f\"  {well}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e473244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 处理LAS文件 ============\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"开始处理LAS文件...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 获取所有LAS文件\n",
    "input_path = Path(LAS_FOLDER)\n",
    "las_files = list(input_path.glob(\"*.las\"))\n",
    "las_files = list(set(las_files))  # 去重\n",
    "las_files.sort()\n",
    "\n",
    "if not las_files:\n",
    "    print(f\"\\n在 {LAS_FOLDER} 中未找到LAS文件\")\n",
    "else:\n",
    "    print(f\"\\n找到 {len(las_files)} 个LAS文件\")\n",
    "\n",
    "    # 统计信息\n",
    "    processing_stats = []\n",
    "    success_count = 0\n",
    "    failed_files = []\n",
    "\n",
    "    for las_file in las_files:\n",
    "        well_name = las_file.stem\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"处理井: {well_name}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "\n",
    "        try:\n",
    "            # 读取LAS文件\n",
    "            las = lasio.read(las_file, mnemonic_case=\"upper\")\n",
    "\n",
    "            # 获取深度数据\n",
    "            depth_curve = None\n",
    "            for mnemonic in [\"DEPT\", \"DEPTH\", \"MD\"]:\n",
    "                if mnemonic in [c.mnemonic for c in las.curves]:\n",
    "                    depth_curve = mnemonic\n",
    "                    break\n",
    "\n",
    "            if depth_curve is None:\n",
    "                print(f\"  ✗ 未找到深度曲线,跳过\")\n",
    "                failed_files.append((well_name, \"无深度曲线\"))\n",
    "                continue\n",
    "\n",
    "            depth_data = las[depth_curve]\n",
    "            print(f\"深度曲线: {depth_curve}\")\n",
    "            print(f\"深度范围: {depth_data.min():.2f}m - {depth_data.max():.2f}m\")\n",
    "\n",
    "            # 统计该井的异常值信息\n",
    "            well_stats = {\n",
    "                \"Well\": well_name,\n",
    "                \"Depth_Min\": depth_data.min(),\n",
    "                \"Depth_Max\": depth_data.max(),\n",
    "                \"Depth_Range\": depth_data.max() - depth_data.min(),\n",
    "                \"Total_Points\": len(depth_data),\n",
    "            }\n",
    "\n",
    "            # 创建新的LAS对象\n",
    "            new_las = lasio.LASFile()\n",
    "            new_las.version = las.version\n",
    "            new_las.well = las.well\n",
    "            new_las.params = las.params\n",
    "            new_las.other = las.other\n",
    "\n",
    "            # 处理每条曲线\n",
    "            for curve in las.curves:\n",
    "                mnemonic = curve.mnemonic\n",
    "                data = curve.data.copy()\n",
    "                original_count = len(data)\n",
    "\n",
    "                # 检查是否有对应的异常值规则\n",
    "                rule = ANOMALY_RULES.get(mnemonic)\n",
    "\n",
    "                if rule is not None:\n",
    "                    print(f\"\\n曲线: {mnemonic}\")\n",
    "                    print(f\"  原始数据点数: {original_count}\")\n",
    "\n",
    "                    # 使用分层检测方法\n",
    "                    total_mask, stats = detect_outliers_by_geological_layers(\n",
    "                        data=data,\n",
    "                        depth=depth_data,\n",
    "                        horizon_df=horizon_df,\n",
    "                        well_name=well_name,\n",
    "                        sand_groups=SAND_GROUPS,\n",
    "                        rule=rule,\n",
    "                        **DETECTION_CONFIG,\n",
    "                        verbose=True,\n",
    "                    )\n",
    "\n",
    "                    # 将异常值设为NaN\n",
    "                    data[total_mask] = np.nan\n",
    "\n",
    "                    # 记录统计信息\n",
    "                    well_stats[f\"{mnemonic}_Points\"] = stats[\"original_count\"]\n",
    "                    well_stats[f\"{mnemonic}_Prior_Count\"] = stats[\"prior_count\"]\n",
    "                    well_stats[f\"{mnemonic}_Prior_Pct\"] = stats[\"prior_pct\"]\n",
    "                    well_stats[f\"{mnemonic}_Statistical_Count\"] = stats[\"statistical_count\"]\n",
    "                    well_stats[f\"{mnemonic}_Statistical_Pct\"] = stats[\"statistical_pct\"]\n",
    "                    well_stats[f\"{mnemonic}_Total_Count\"] = stats[\"total_count\"]\n",
    "                    well_stats[f\"{mnemonic}_Total_Pct\"] = stats[\"total_pct\"]\n",
    "                    well_stats[f\"{mnemonic}_Valid_Count\"] = stats[\"valid_count\"]\n",
    "                    well_stats[f\"{mnemonic}_Layers_Processed\"] = stats[\"layers_processed\"]\n",
    "                    well_stats[f\"{mnemonic}_Layers_Skipped\"] = stats[\"layers_skipped\"]\n",
    "\n",
    "                else:\n",
    "                    # 对于没有先验规则的曲线,只进行基本的NaN和Inf检测\n",
    "                    basic_mask = np.isnan(data) | np.isinf(data)\n",
    "                    basic_anomaly_count = np.sum(basic_mask)\n",
    "\n",
    "                    if basic_anomaly_count > 0:\n",
    "                        print(f\"\\n曲线: {mnemonic}\")\n",
    "                        print(f\"  原始数据点数: {original_count}\")\n",
    "                        print(f\"  NaN/Inf异常: {basic_anomaly_count}\")\n",
    "                        data[basic_mask] = np.nan\n",
    "\n",
    "                # 添加处理后的曲线\n",
    "                new_las.append_curve(\n",
    "                    mnemonic=curve.mnemonic,\n",
    "                    data=data,\n",
    "                    unit=curve.unit,\n",
    "                    descr=curve.descr,\n",
    "                    value=curve.value,\n",
    "                )\n",
    "\n",
    "            # 保存处理后的文件\n",
    "            output_file = output_path / las_file.name\n",
    "            new_las.write(str(output_file), version=2.0)\n",
    "\n",
    "            print(f\"\\n✓ 成功处理并保存到 {output_file.name}\")\n",
    "            success_count += 1\n",
    "\n",
    "            # 保存统计信息\n",
    "            processing_stats.append(well_stats)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ 处理失败: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "            failed_files.append((well_name, str(e)))\n",
    "\n",
    "    # ============ 保存处理统计信息 ============\n",
    "\n",
    "    if processing_stats:\n",
    "        stats_df = pd.DataFrame(processing_stats)\n",
    "        stats_file = \"output/processing_statistics_by_layers.xlsx\"\n",
    "        stats_df.to_excel(stats_file, index=False)\n",
    "        print(f\"\\n统计信息已保存到: {stats_file}\")\n",
    "\n",
    "    # ============ 输出总体统计 ============\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"处理完成!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"成功: {success_count}/{len(las_files)}\")\n",
    "\n",
    "    if failed_files:\n",
    "        print(f\"\\n失败的文件:\")\n",
    "        for filename, error in failed_files:\n",
    "            print(f\"  - {filename}: {error}\")\n",
    "\n",
    "    # ============ 可视化统计结果 ============\n",
    "\n",
    "    if processing_stats:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"生成统计图表...\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        stats_df = pd.DataFrame(processing_stats)\n",
    "\n",
    "        # 统计各曲线的异常值情况\n",
    "        curves_to_analyze = [\"GR\", \"DEN\", \"RHOB\", \"DT\", \"AC\", \"CAL\", \"CALI\", \"LLD\", \"LLD1\", \"POR\"]\n",
    "\n",
    "        # 提取异常值百分比数据\n",
    "        anomaly_pct_data = {}\n",
    "        for curve in curves_to_analyze:\n",
    "            pct_col = f\"{curve}_Total_Pct\"\n",
    "            if pct_col in stats_df.columns:\n",
    "                anomaly_pct_data[curve] = stats_df[pct_col].fillna(0).values\n",
    "\n",
    "        # 创建对比图表\n",
    "        if anomaly_pct_data:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "            # 1. 热力图\n",
    "            ax1 = axes[0]\n",
    "            pct_df = pd.DataFrame(anomaly_pct_data, index=stats_df[\"Well\"])\n",
    "            pct_df_filtered = pct_df.loc[:, (pct_df > 0).any()]\n",
    "\n",
    "            if len(pct_df_filtered.columns) > 0:\n",
    "                im = ax1.imshow(pct_df_filtered.values, cmap=\"YlOrRd\", aspect=\"auto\", vmin=0, vmax=100)\n",
    "                ax1.set_xticks(range(len(pct_df_filtered.columns)))\n",
    "                ax1.set_xticklabels(pct_df_filtered.columns, fontsize=11, fontweight=\"bold\")\n",
    "                ax1.set_yticks(range(len(pct_df_filtered.index)))\n",
    "                ax1.set_yticklabels(pct_df_filtered.index, fontsize=10)\n",
    "                ax1.set_title(\"各井各曲线异常值占比热力图 (%)\\n(分层处理)\", fontweight=\"bold\", fontsize=13, pad=15)\n",
    "                ax1.set_xlabel(\"曲线类型\", fontsize=11, fontweight=\"bold\")\n",
    "                ax1.set_ylabel(\"井名\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "                cbar = plt.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
    "                cbar.set_label(\"异常值占比 (%)\", rotation=270, labelpad=20, fontsize=10)\n",
    "\n",
    "                # 标注数值\n",
    "                for i in range(len(pct_df_filtered.index)):\n",
    "                    for j in range(len(pct_df_filtered.columns)):\n",
    "                        value = pct_df_filtered.values[i, j]\n",
    "                        if value > 0:\n",
    "                            text_color = \"white\" if value > 50 else \"black\"\n",
    "                            ax1.text(\n",
    "                                j,\n",
    "                                i,\n",
    "                                f\"{value:.1f}\",\n",
    "                                ha=\"center\",\n",
    "                                va=\"center\",\n",
    "                                color=text_color,\n",
    "                                fontsize=8,\n",
    "                                fontweight=\"bold\",\n",
    "                            )\n",
    "\n",
    "            # 2. 柱状图 - 比较各井的异常值占比\n",
    "            ax2 = axes[1]\n",
    "            if len(pct_df_filtered.columns) > 0:\n",
    "                pct_df_filtered.plot(kind=\"bar\", ax=ax2, width=0.8, colormap=\"tab10\")\n",
    "                ax2.set_title(\"各井各曲线异常值占比\\n(分层处理)\", fontweight=\"bold\", fontsize=13)\n",
    "                ax2.set_xlabel(\"井名\", fontsize=11)\n",
    "                ax2.set_ylabel(\"异常值占比 (%)\", fontsize=11)\n",
    "                ax2.legend(title=\"曲线\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=9)\n",
    "                ax2.set_xticklabels(pct_df_filtered.index, rotation=45, ha=\"right\", fontsize=10)\n",
    "                ax2.grid(axis=\"y\", alpha=0.3)\n",
    "                ax2.set_ylim(0, max(100, pct_df_filtered.max().max() * 1.1))\n",
    "\n",
    "            fig.suptitle(\"测井曲线异常值统计分析（分层处理方法）\", fontsize=15, fontweight=\"bold\", y=0.98)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # 保存图表\n",
    "            fig_file = \"output/anomaly_statistics_by_layers.png\"\n",
    "            plt.savefig(fig_file, dpi=300, bbox_inches=\"tight\")\n",
    "            print(f\"统计图表已保存到: {fig_file}\")\n",
    "\n",
    "        # ============ 输出统计摘要 ============\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"异常值统计摘要（分层处理）\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        if anomaly_pct_data:\n",
    "            pct_df = pd.DataFrame(anomaly_pct_data, index=stats_df[\"Well\"])\n",
    "            print(\"\\n各曲线平均异常值占比:\")\n",
    "            for col in pct_df.columns:\n",
    "                if pct_df[col].sum() > 0:\n",
    "                    avg_pct = pct_df[col].mean()\n",
    "                    max_pct = pct_df[col].max()\n",
    "                    max_well = pct_df[col].idxmax()\n",
    "                    print(f\"  {col:8s}: 平均 {avg_pct:5.2f}%  最大 {max_pct:5.2f}% ({max_well})\")\n",
    "\n",
    "            print(\"\\n异常值占比最高的5口井:\")\n",
    "            well_avg = pct_df.mean(axis=1).sort_values(ascending=False).head(5)\n",
    "            for well, avg in well_avg.items():\n",
    "                print(f\"  {well:20s}: {avg:5.2f}%\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ 所有处理完成!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transform2021_devito",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
